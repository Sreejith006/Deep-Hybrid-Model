{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 1: LIBRARY IMPORTS\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Import os for path joining\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    MaxPooling1D,\n",
    "    Dropout,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    GlobalAveragePooling1D,\n",
    "    SeparableConv1D \n",
    ")\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "# Ensure reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: DATA LOADING AND PREPROCESSING (CRITICAL PATH FIXED)\n",
    "# ==============================================================================\n",
    "\n",
    "# ðŸ›‘ PATH FIX APPLIED: Using 'r' prefix for raw strings to ignore escape sequences ðŸ›‘\n",
    "# We are now using ABSOLUTE paths, so PATH_TO_DATA_FOLDER is no longer needed.\n",
    "base_file_names = [\n",
    "    r'C:\\Users\\sreej\\OneDrive\\Desktop\\Deep Hybrid Model\\backend\\dataset\\singlehop-indoor-moteid1-data.txt',\n",
    "    r'C:\\Users\\sreej\\OneDrive\\Desktop\\Deep Hybrid Model\\backend\\dataset\\singlehop-indoor-moteid2-data.txt',\n",
    "    r'C:\\Users\\sreej\\OneDrive\\Desktop\\Deep Hybrid Model\\backend\\dataset\\singlehop-outdoor-moteid3-data.txt',\n",
    "    r'C:\\Users\\sreej\\OneDrive\\Desktop\\Deep Hybrid Model\\backend\\dataset\\singlehop-outdoor-moteid4-data.txt'\n",
    "]\n",
    "\n",
    "# FIX: file_paths is now just the list of absolute paths\n",
    "file_paths = base_file_names\n",
    "\n",
    "COLUMNS = ['Reading_Num', 'Mote_ID', 'Humidity', 'Temperature', 'Label']\n",
    "all_mote_data = []\n",
    "\n",
    "def load_and_clean_mote_data(file_path, skip_rows):\n",
    "    \"\"\"\n",
    "    Loads and cleans a single mote data file by skipping junk rows \n",
    "    and manually assigning the known columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load data\n",
    "        # sep=r'\\s+' handles both tabs and multiple spaces as delimiters\n",
    "        df = pd.read_csv(\n",
    "            file_path, \n",
    "            sep=r'\\s+',              \n",
    "            header=None,             \n",
    "            skiprows=skip_rows,      \n",
    "            engine='python',         \n",
    "            usecols=range(5),        \n",
    "            on_bad_lines='skip',\n",
    "            encoding='utf-8' \n",
    "        )\n",
    "        \n",
    "        # 2. MANUALLY ASSIGN THE CORRECT COLUMN NAMES (Ensures 'Label' exists)\n",
    "        df.columns = COLUMNS\n",
    "        \n",
    "        # 3. Robust type conversion and cleaning\n",
    "        for col in ['Reading_Num', 'Mote_ID', 'Label']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "        \n",
    "        for col in ['Humidity', 'Temperature']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # 4. Drop rows where critical data is missing\n",
    "        df.dropna(subset=['Mote_ID', 'Humidity', 'Temperature', 'Label'], inplace=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} rows from {os.path.basename(file_path)}\")\n",
    "        return df.astype({'Mote_ID': int, 'Label': int}) \n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error processing {file_path}: [Errno 2] File Not Found. Double-check your absolute C: path.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        # This catches residual issues like column reading errors, which the manual column assignment should fix.\n",
    "        print(f\"Error processing {file_path}. Data not included: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Mote-specific loading using the required skiprows for this dataset:\n",
    "skip_rules = {\n",
    "    'singlehop-indoor-moteid1-data.txt': 4,\n",
    "    'singlehop-indoor-moteid2-data.txt': 1,\n",
    "    'singlehop-outdoor-moteid3-data.txt': 1,\n",
    "    'singlehop-outdoor-moteid4-data.txt': 2\n",
    "}\n",
    "\n",
    "# Use the full path for loading, but the basename for the skip_rules lookup\n",
    "for full_path in file_paths:\n",
    "    # Use the filename (basename) to find the correct skip rule\n",
    "    file_name_only = os.path.basename(full_path)\n",
    "    all_mote_data.append(load_and_clean_mote_data(full_path, skip_rules.get(file_name_only, 0))) # Default to 0 if key not found\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(all_mote_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal merged data points: {len(merged_df)}\")\n",
    "if len(merged_df) > 0:\n",
    "    print(f\"Fault distribution:\\n{merged_df['Label'].value_counts(normalize=True).mul(100).round(2)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: MODEL DEFINITION (EFFICIENT-TransCNN / TransCNN-Lite)\n",
    "# (REMAINS UNCHANGED)\n",
    "# ==============================================================================\n",
    "\n",
    "if len(merged_df) > 0:\n",
    "    # --- Sequence Creation ---\n",
    "    TIME_STEP = 50  # The sequence length (or look-back window)\n",
    "    FEATURES = ['Humidity', 'Temperature']\n",
    "    N_FEATURES = len(FEATURES)\n",
    "\n",
    "    X = merged_df[FEATURES].values\n",
    "    y = merged_df['Label'].values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    def create_sequences(X, y, time_steps):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            Xs.append(X[i:(i + time_steps)])\n",
    "            ys.append(y[i + time_steps]) \n",
    "        return np.array(Xs), np.array(ys)\n",
    "\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y, TIME_STEP)\n",
    "    print(f\"\\nSequence input shape (N_samples, Time_Steps, N_Features): {X_seq.shape}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
    "    )\n",
    "    print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "\n",
    "    def build_efficient_transcnn(input_shape, d_model=64, num_heads=4):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        \n",
    "        # 1. Efficient CNN Block\n",
    "        x = SeparableConv1D(filters=d_model, kernel_size=5, padding='same', activation='relu', depth_multiplier=1, name='Depthwise_CNN_Block')(inputs)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        # 2. Transformer/Attention Block\n",
    "        attn_output = tf.keras.layers.MultiHeadAttention(key_dim=d_model, num_heads=num_heads, dropout=0.1, name='MHA_Block')(x, x)\n",
    "        x = x + attn_output\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        ffn_output = Dense(d_model * 2, activation=\"relu\")(x)\n",
    "        ffn_output = Dense(d_model)(ffn_output)\n",
    "        ffn_output = Dropout(0.1)(ffn_output)\n",
    "        \n",
    "        x = x + ffn_output\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # 3. Classifier Block\n",
    "        z = GlobalAveragePooling1D()(x) \n",
    "        z = Dropout(0.5)(z)\n",
    "        \n",
    "        outputs = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='Efficient_TransCNN')\n",
    "        return model\n",
    "\n",
    "    # Initialize and Compile\n",
    "    model = build_efficient_transcnn(\n",
    "        input_shape=(TIME_STEP, N_FEATURES), \n",
    "        d_model=64,\n",
    "        num_heads=4\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=[ 'accuracy', Precision(name='precision'), Recall(name='recall') ]\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Efficient-TransCNN Model Summary ---\")\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # SECTION 4: MODEL TRAINING AND EVALUATION\n",
    "    # (REMAINS UNCHANGED)\n",
    "    # ==============================================================================\n",
    "\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "    # Training Parameters\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 64\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1, \n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluation on Test Data\n",
    "    print(\"\\n--- Evaluating Model Performance ---\")\n",
    "\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    loss, acc, precision, recall = results\n",
    "\n",
    "    # Calculate F1-Score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "    # Classification Report and Confusion Matrix\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "    print(\"\\n--- Detailed Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal (0)', 'Fault (1)']))\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save('efficient_transcnn_fault_diagnosis.h5')\n",
    "    print(\"\\nModel saved as 'efficient_transcnn_fault_diagnosis.h5'\")\n",
    "else:\n",
    "    print(\"FATAL: Data loading failed. Please verify the absolute paths in 'base_file_names' list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39897e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 6: VISUALIZATION (Plot Training History)\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt # <--- THE FIX IS HERE\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: This assumes 'history' variable from Section 4 is still in memory.\n",
    "# If you closed your notebook, you must re-run the training section (Section 4) \n",
    "# to recreate the 'history' variable before running this cell!\n",
    "\n",
    "try:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # --- Plot 1: Loss ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss Over Epochs (Convergence)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (Binary Crossentropy)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # --- Plot 2: Accuracy/F1-Metrics ---\n",
    "    \n",
    "    # Custom function to calculate F1 from P & R in history\n",
    "    # The array operations need to handle potential division by zero if P+R=0\n",
    "    \n",
    "    # Calculate Training F1-Score\n",
    "    precision_train = np.array(history.history['precision'])\n",
    "    recall_train = np.array(history.history['recall'])\n",
    "    f1_train = np.divide(2 * precision_train * recall_train, \n",
    "                         precision_train + recall_train, \n",
    "                         out=np.zeros_like(precision_train), \n",
    "                         where=(precision_train + recall_train) != 0)\n",
    "\n",
    "    # Calculate Validation F1-Score\n",
    "    precision_val = np.array(history.history['val_precision'])\n",
    "    recall_val = np.array(history.history['val_recall'])\n",
    "    f1_val = np.divide(2 * precision_val * recall_val, \n",
    "                       precision_val + recall_val, \n",
    "                       out=np.zeros_like(precision_val), \n",
    "                       where=(precision_val + recall_val) != 0)\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linestyle='--')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linestyle='--')\n",
    "    plt.plot(f1_train, label='Train F1-Score', linewidth=2)\n",
    "    plt.plot(f1_val, label='Validation F1-Score', linewidth=2)\n",
    "    \n",
    "    plt.title('Model Performance Over Epochs (F1-Score is Key)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"FATAL ERROR: 'history' variable not found. You must run the training cell (Section 4) first!\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during plotting: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 7: SINGLE-SAMPLE PREDICTION CHECK\n",
    "# ==============================================================================\n",
    "# Use one sample from the test set\n",
    "sample_index = 0 \n",
    "sample_X = X_test[sample_index].reshape(1, X_test.shape[1], X_test.shape[2])\n",
    "sample_Y_true = y_test[sample_index]\n",
    "\n",
    "# Predict the sample\n",
    "prediction_proba = model.predict(sample_X)[0][0]\n",
    "prediction_class = int(prediction_proba > 0.5)\n",
    "\n",
    "print(f\"--- Single Sample Prediction Check (Index {sample_index}) ---\")\n",
    "print(f\"Actual Label (0=Normal, 1=Fault): {sample_Y_true}\")\n",
    "print(f\"Predicted Probability of Fault (1): {prediction_proba:.4f}\")\n",
    "print(f\"Predicted Class: {prediction_class}\")\n",
    "\n",
    "if prediction_class == sample_Y_true:\n",
    "    print(\"Result: Correctly classified!\")\n",
    "else:\n",
    "    print(\"Result: Misclassified. Analyze this sample for robustness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7da1bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully reconstructed TransCNN model architecture.\n",
      "Successfully loaded weights from efficient_transcnn_fault_diagnosis.h5 into the new model structure.\n",
      "\n",
      "Model successfully saved to efficient_transcnn_fixed.h5. It is now ready for TFLite conversion.\n",
      "Next step: Re-run the 'Efficient-TransCNN TFLite Converter' Canvas.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MODEL RECONSTRUCTION AND WEIGHT LOADING SCRIPT\n",
    "# FIX: Adjusted final dense layer to Dense(1, activation='sigmoid') and loss\n",
    "# to 'binary_crossentropy' to match the likely structure of the original saved model\n",
    "# trained for binary classification.\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Conv1D, DepthwiseConv1D, Activation, Add, GlobalAveragePooling1D, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define Custom Layers (Must be defined first) ---\n",
    "\n",
    "class PositionalEmbedding(Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding layer used in the Transformer part of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, d_model, *args, **kwargs):\n",
    "        if args and 'name' not in kwargs:\n",
    "             kwargs['name'] = args[0]\n",
    "        super().__init__(**kwargs) \n",
    "        self.sequence_length = sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=d_model\n",
    "        )\n",
    "        self.token_embeddings = tf.keras.layers.Conv1D(d_model, 1, padding=\"valid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"sequence_length\": self.sequence_length, \"d_model\": self.d_model})\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    \"\"\"\n",
    "    Standard Transformer Block composed of multi-head self-attention and a feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1, *args, **kwargs): \n",
    "        if args and 'name' not in kwargs:\n",
    "             kwargs['name'] = args[0]\n",
    "        super().__init__(**kwargs) \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(d_model)]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model, \"num_heads\": self.num_heads, \"ff_dim\": self.ff_dim, \"rate\": self.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# --- 2. Define the Complete TransCNN Model Architecture ---\n",
    "# NOTE: These parameters (sequence_length, num_features, etc.) MUST match \n",
    "# what was used when the model was originally trained. Please verify!\n",
    "\n",
    "def build_transcnn_model(sequence_length=40, num_features=2, d_model=64, num_heads=4, ff_dim=128):\n",
    "    \"\"\"\n",
    "    Builds the TransCNN hybrid model architecture.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(sequence_length, num_features))\n",
    "\n",
    "    # --- CNN Feature Extraction Block ---\n",
    "    x = Conv1D(filters=32, kernel_size=8, strides=1, activation='relu', padding='same')(inputs)\n",
    "    x = Conv1D(filters=d_model, kernel_size=8, strides=1, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # --- Transformer Block ---\n",
    "    # Positional Embedding layer\n",
    "    x = PositionalEmbedding(sequence_length=sequence_length, d_model=d_model)(x)\n",
    "    \n",
    "    # Transformer Block\n",
    "    x = TransformerBlock(d_model=d_model, num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "\n",
    "    # --- Classification Head ---\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(64, activation='relu')(x) \n",
    "    \n",
    "    # FIX: Using 1 output neuron with sigmoid activation for binary classification\n",
    "    outputs = Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"TransCNN_Fault_Diagnosis\")\n",
    "    return model\n",
    "\n",
    "# --- 3. Instantiate, Compile, and Load Weights ---\n",
    "\n",
    "MODEL_FILE = 'efficient_transcnn_fault_diagnosis.h5'\n",
    "NEW_MODEL_FILE = 'efficient_transcnn_fixed.h5'\n",
    "# The final layer is now fixed to 1 neuron, so we don't need NUM_CLASSES explicitly in the rebuild function\n",
    "SEQUENCE_LENGTH = 40 # Assuming your sequence length for input data\n",
    "NUM_FEATURES = 2 # Assuming Humidity and Temperature (2 features)\n",
    "\n",
    "try:\n",
    "    # 3.1. Build the model architecture from scratch (Note: num_classes removed)\n",
    "    new_model = build_transcnn_model(\n",
    "        sequence_length=SEQUENCE_LENGTH, \n",
    "        num_features=NUM_FEATURES, \n",
    "    )\n",
    "    \n",
    "    # 3.2. Compile the model (MUST match original training setup)\n",
    "    new_model.compile(\n",
    "        optimizer='adam', \n",
    "        # FIX: Using binary_crossentropy for the 1-neuron sigmoid output layer\n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully reconstructed TransCNN model architecture.\")\n",
    "\n",
    "    # 3.3. Load ONLY the weights from the incompatible file\n",
    "    new_model.load_weights(MODEL_FILE, by_name=True)\n",
    "    print(f\"Successfully loaded weights from {MODEL_FILE} into the new model structure.\")\n",
    "\n",
    "    # 3.4. Re-save the model in a fully compatible format\n",
    "    new_model.save(NEW_MODEL_FILE)\n",
    "    print(f\"\\nModel successfully saved to {NEW_MODEL_FILE}. It is now ready for TFLite conversion.\")\n",
    "    print(\"Next step: Re-run the 'Efficient-TransCNN TFLite Converter' Canvas.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL FAILURE: Failed to reconstruct model or load weights. Error: {e}\")\n",
    "    print(\"If the error persists, there may be another layer mismatch. We may need to adjust the intermediate Dense(64) layer or the CNN block.\")\n",
    "    print(\"Please verify your original model building code if possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240c1bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Keras model from efficient_transcnn_fixed.h5.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\sreej\\AppData\\Local\\Temp\\tmphn7ph7sw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\sreej\\AppData\\Local\\Temp\\tmphn7ph7sw\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\sreej\\AppData\\Local\\Temp\\tmphn7ph7sw'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 40, 2), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2432776806736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776807120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776808080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776807312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776808656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776808272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776806928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776808848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776809040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776809424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776809616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776810192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776806160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776810576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776808464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776811152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776806352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776809808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776810960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776811728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776811536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776812304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776811344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776812112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776812496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776810384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2432776810768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "SUCCESS: TFLite model saved to efficient_transcnn.tflite\n",
      "The size of the converted model is: 147.05 KB.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TFLITE CONVERSION SCRIPT\n",
    "# Loads the fixed Keras model and converts it to a TensorFlow Lite model.\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. Define Custom Layers (REQUIRED for Keras Model Loading) ---\n",
    "# We must include the custom layer definitions again so Keras can correctly \n",
    "# load the architecture from the saved .h5 file.\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding layer used in the Transformer part of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, d_model, *args, **kwargs):\n",
    "        # Handle positional arguments in legacy Keras format\n",
    "        if args and 'name' not in kwargs:\n",
    "             kwargs['name'] = args[0]\n",
    "        super().__init__(**kwargs) \n",
    "        self.sequence_length = sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=d_model\n",
    "        )\n",
    "        self.token_embeddings = tf.keras.layers.Conv1D(d_model, 1, padding=\"valid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"sequence_length\": self.sequence_length, \"d_model\": self.d_model})\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Standard Transformer Block composed of multi-head self-attention and a feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, ff_dim, rate=0.1, *args, **kwargs): \n",
    "        # Handle positional arguments in legacy Keras format\n",
    "        if args and 'name' not in kwargs:\n",
    "             kwargs['name'] = args[0]\n",
    "        super().__init__(**kwargs) \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(d_model)]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model, \"num_heads\": self.num_heads, \"ff_dim\": self.ff_dim, \"rate\": self.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# --- 2. Define Conversion Parameters ---\n",
    "\n",
    "KERAS_MODEL_PATH = 'efficient_transcnn_fixed.h5'\n",
    "TFLITE_MODEL_PATH = 'efficient_transcnn.tflite'\n",
    "# The input shape of the model, determined by the original training script\n",
    "# (Batch size, Sequence Length, Features)\n",
    "INPUT_SHAPE = (1, 40, 2) \n",
    "\n",
    "\n",
    "# --- 3. Conversion Function ---\n",
    "\n",
    "def representative_dataset_generator():\n",
    "    \"\"\"\n",
    "    A generator function to provide representative data samples for \n",
    "    Post-Training Integer Quantization. \n",
    "    NOTE: Replace the dummy data generation below with actual preprocessed data\n",
    "    from your dataset if you want to use full integer quantization.\n",
    "    For now, we'll use dummy data for basic conversion.\n",
    "    \"\"\"\n",
    "    # Using dummy data matching the expected input shape (40 steps, 2 features)\n",
    "    for _ in range(10): # Generate 10 dummy samples\n",
    "        yield [np.random.rand(*INPUT_SHAPE).astype(np.float32)]\n",
    "\n",
    "def convert_to_tflite(keras_model_path, tflite_model_path):\n",
    "    \"\"\"\n",
    "    Loads the Keras model and converts it to TFLite format.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(keras_model_path):\n",
    "        print(f\"ERROR: Keras model file not found at {keras_model_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the fixed Keras model, passing the custom objects\n",
    "        custom_objects = {\n",
    "            'PositionalEmbedding': PositionalEmbedding,\n",
    "            'TransformerBlock': TransformerBlock\n",
    "        }\n",
    "        model = tf.keras.models.load_model(\n",
    "            keras_model_path, \n",
    "            custom_objects=custom_objects\n",
    "        )\n",
    "        print(f\"Successfully loaded Keras model from {keras_model_path}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR: Failed to load the Keras model for conversion. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Initialize the TFLite converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # --- Optimization Settings (Uncomment for desired optimization level) ---\n",
    "    \n",
    "    # 1. Default Optimization (Size reduction, generally safe)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    # 2. Integer Quantization (For highest memory and computational saving)\n",
    "    # This requires providing representative data.\n",
    "    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # converter.representative_dataset = representative_dataset_generator\n",
    "    # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    # converter.inference_input_type = tf.int8  # Optional: depends on deployment\n",
    "    # converter.inference_output_type = tf.int8 # Optional: depends on deployment\n",
    "\n",
    "    try:\n",
    "        # Perform the conversion\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the TFLite model\n",
    "        with open(tflite_model_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"\\nSUCCESS: TFLite model saved to {tflite_model_path}\")\n",
    "        print(f\"The size of the converted model is: {len(tflite_model) / 1024:.2f} KB.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR during TFLite conversion: {e}\")\n",
    "        print(\"This often happens due to incompatibility of custom layers with TFLite. Ensure all custom layer operations are TFLite-compatible.\")\n",
    "\n",
    "# --- 4. Execute Conversion ---\n",
    "if __name__ == '__main__':\n",
    "    convert_to_tflite(KERAS_MODEL_PATH, TFLITE_MODEL_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
